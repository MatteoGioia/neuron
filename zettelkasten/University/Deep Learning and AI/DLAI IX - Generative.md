# Deep generative models

### Recall questions

Also see:
- big data...
- notes on entropy and divergence

1. <details markdown=1><summary markdown="span"> What is the idea behind generative models ? </summary>
    
    \
    We want to ==learn a distribution from some training samples, and generate new samples from the same distribution==.
  
</details>

2. <details markdown=1><summary markdown="span"> How can we use PCA as a generative model? </summary>
    
    \

</details>

3. <details markdown=1><summary markdown="span"> What is an autoencoder? What does it minimize? </summary>
    
    \
  

</details>

4. <details markdown=1><summary markdown="span"> What is the goal of an autoencoder? How do we ensure that no trivial solution is provided? </summary>
    
    \
  

</details>

5. <details markdown=1><summary markdown="span"> What can be said about the latent space and embedding in autoencoders? </summary>
    
    \
    ... \
    Latent space is ... while data embedding space is ..

</details>

6. <details markdown=1><summary markdown="span"> How do you define a manifold? What are the key requirements for charts in order to be manifolds? </summary>
    
    \
  

</details>

7. <details markdown=1><summary markdown="span"> Is the parametrization for an embedding unique? </summary>
    
    \
  

</details>

8. <details markdown=1><summary markdown="span"> What is the link between an AE and a manifold then? </summary>
    
    \
    The decoder..

</details>

9. <details markdown=1><summary markdown="span"> What is the issue with creating a probabilistic encoder "from scratch"? </summary>
    
    \
  

</details>

10. <details markdown=1><summary markdown="span"> ? </summary>
    
    \
  

</details>