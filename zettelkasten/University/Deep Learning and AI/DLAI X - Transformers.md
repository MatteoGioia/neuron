# Transformers

### Recall questions

1. <details markdown=1><summary markdown="span"> What is a seq2seq model? What fundamental property must be satisfied?</summary>

    \

</details>

2. <details markdown=1><summary markdown="span"> What is the difference between causal and non-causal layers?</summary>

    \

</details>

3. <details markdown=1><summary markdown="span"> What is the basic idea behind auto-regressive modeling?</summary>

    \

</details>

4. <details markdown=1><summary markdown="span"> What are possible architectures of seq2seq layers? What is the problem? Can we fix it?</summary>

    \
	Two examples:

</details>

5. <details markdown=1><summary markdown="span"> Can you explain how a simple self attention mechanism works? Why we could say that we are not learning any "parameter", in a sense?</summary>

    \

</details>

6. <details markdown=1><summary markdown="span"> Self attention has an interesting property, what is it? Why could it detrimental in some cases?</summary>

    \

</details>

7. <details markdown=1><summary markdown="span"> How do we make self attention parameters "learnable"?</summary>

    \

</details>

8. <details markdown=1><summary markdown="span"> How do we fix the problem mentioned in 6? How is this technique called?</summary>

    \

</details>

9. <details markdown=1><summary markdown="span"> ?</summary>

    \

</details>

10. <details markdown=1><summary markdown="span"> What is a transformer then? </summary>

    \

</details>