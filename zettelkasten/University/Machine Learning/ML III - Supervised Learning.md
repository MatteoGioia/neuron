# Supervised learning

## Recall questions
      - How is the supervised ML task defined? What are the differences between classifiers and regressors?
      - What is the purpose of splitting the dataset?
      - What is the structure of a DT? How can we rewrite it? (Hint: boolean formula or set of rules)
      - What is the primary method of building a DT? 
      - How can we handle missing data when training, e.g. deciding for a feature value not in the training set?
      - Is picking a good split hard? Is the chosen method an hyperparameter?
      - What is the information gain? How is it linked to the entropy?
      - What are support and confidence?
      - What are the main issues of DT?
      - What are the main differences between DT and RT? What is the goal in RT?
      - Describe the technique used to choose the best split at each iteration.
      - What are the parameters we tune in DT and RT?
      - What is overfitting? Why is it bad?
      - What is pruning? What are the main techniques and what are the main drawbacks?


## Classifiers and Regressors

### Definition of the task

==Let $D$ be a dataset representing historical data about a given domain==. $D$ has a list of
==records==, called ==instances $x_i : (x_{i1}, \ldots, x_{in})$==

Each ==$x_ij$ represents a value of an attribute/feature==
- can be either continous or discrete
- one of them is the target we want to learn a prediction model for

Target is denoted with:
- ==$f(x)$ if it's a continous value==, so the model is a ==regressor==
- ==$c(x)$ if it's a discrete value==, so the model is a ==classifier==

### Training the model

Dataset is split into 2 parts:
- the ==training set==
- the ==test set==

At the end of the process, the ==created model is tested on the test set to see if the performance is accettable==.
- if not, some tuning can be done

### Geometric representation

Records can be seen as vectors in a multidimensional space

![](../../../ML/toydataset.png)

## Classification

### Decision trees

The model generated by a decision tree is a ==tree structure==. The ==root and intermediate nodes are tests on the feature values==, with as many ==branches as the possible feature values==, while each ==leaf is a decision==, so a class.

![](../../../ML/dt1.png)

A ==DT can represent any boolean function $c(x)$==. It ==can also be rewritten as a set of rules==, i.e. in Disjunctive Normal Form.

### Building a DT

The tree is built with ==greedy recursive partition== of the decision space:
- the algorithm considers each feature and builds a test node for each one
- for each test node, a new branch is created when a new feature value appears

![](../../../ML/dt2.png)

2 key aspects of building a DT (examined later on):
- ==decide for nodes for which we have 0 examples==
- learning ==best split==

The second aspect, together with learning the best ordering of the tests on the features, is actually
==learning the DT paraemeters==.

### DT pseudocode

```
   DTree(examples D, features F)
      if(all examples D are in the same category)
         return a leaf node with that category label
      else if (set of features F is empty)
         return a leaf node with the category label that is the most common in the examples subset

      Else(pick f in F)
         create node R for f
         for each(value x_i in f)
            let s_i = subset examples with value x_i for f
            add outgoing edge E from R labelled with the value of x_i
            if(s_i == empty)
               attach a leaf node to E
               label E with the category that is the most common in the examples subset
            else
               T = DTree(s_i, features F - {f})
               attach T to R
   
   Return the subtree rooted in R

```

So the first issue is solved: ==when no prior example is given, the most common category among the examples is assigned==
- in case of a tie its random

![](../../../ML/dt3.png)

We still need to see how to pick a good split.

### Information gain

Choosing ==which feature to evaluate first is key to building a minimal tree. But finding the minimal decision tree is an NP-hard problem==.
- the method used for this choice is an ==hyper-parameter==

One solution is to use ==information gain==. The idea of information gain is to ==choose first feature with high "purity"==, or for which, in simpler terms, the classification is easier.

![](../../../ML/purity.png)

==Information gain is strongly linked to entropy==:
- for example entropy is equal to 0 if all object belong to the same category
- a mixed set, instead, has entropy 1 if the samples are equally split into 2 categories

Entropy general formula: $E(D) = - \sum_{i=0}^{C} p_i log_2 (p_i)$

==Information Gain $IG$ on a feature $f$== is the ==expected reduction in entropy resulting from splitting on this feature==: $IG(D,f) =  E(D) - \sum_{v \in Values(f)} \frac{|D_v|}{D} E(D_v)$
- where $D_v$ is the subset of $D$ having value $v$ for feature $f$ (i.e. $f$=color and $v$=red)
- and the entropy of each value is weighted against its probability of appearing in the dataset

### DT Pseudocode with IG

```
   DTree(examples D, features F)
      if(all examples D are in one category)
         return a leaf node with that category label
      else if (set of features F is empty)
         return a leaf node with the category label that is the most common in the examples subset

      Else(pick the best f in F according to IG)
         create node R for f
         for each(value x_i in f)
            let s_i = subset examples with value x_i for f
            add outgoing edge E from R labelled with the value of x_i
            if(s_i == empty)
               attach a leaf node to E
               label E with the category that is the most common in the examples subset
            else
               T = DTree(s_i, features F - {f})
               attach T to R
   
   Return the subtree rooted in R

```

Notice that in the subsquent iterations the IG is calculated over the dataset where only the remaining features are taken into account

### A working example

![](../../../ML/dt5.png)

This dataset is associated with this DT. The corrisponding set of rules can also be derived.

![](../../../ML/dt6.png)

### Support and confidence

When we consider the set of rules associated with a DT, we can also look at the:
- ==support== $\frac{|D_v|}{D}$ or ==coverage represented by the set of examples that satisfy the rule==
- ==confidence==, the ==subset of $D_v$ correctly classified by the rule, which might or might not be equal to the support==

For instance, we there is no data for a set of features, if the distribution of $D_v$ is not uniform but, say, $D_{v}^{+}$ values are positive and $D_{v}^{-}$ are negative, with more positives than negatives, the support and confidence of the outputted rule will both be $\frac{|D_{v}^{+}|}{D}$

### Issues of DT

1. ==Can handle continous values but after discretization==
2. ==Only efficient== for ==large amount of training data== for data mining tasks
3. ==Noisy training data and missing value need to be handled with care==

## Regression

### Regression trees

The ==main difference== with DT is:
- ==values can be continous, even the output== (so they are regressors)
- the ==splitting is binary==
 
![](../../../ML/RT1.png)

The ==main goal in RT is to find boxes $R_1 \ldots R_j$ (regions) that minimize the 
Residual Sum of Squares (RSS) = $\sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \cap{y}_{R_j})^2$==
- where $\hat{y}_{R_j}$ is the mean observed value of the training samples whitin the j-th box, and $y_i$ is the value of each single observation in the box

Again, ==finding the best split is an NP-hard problem==, so a more efficient technique is necessary.

### Recursive Binary Splitting

==Recursive binary splitting is a top-down greedy approach that aims to give the best split each time==.

To create a box, ==we consider each value of a continous ordinal variable (feature) with $m$ distinct values, $x_1, \ldots, x_m$ in our learning set==. This way, we can create ==$m-1$ possible partitions== to evaluate the best one.

After calculating the $RSS$ of each square, ==the total $RSS$ is computed and the square that maximizes $RSS_{tot} - RSS_{split}$ is chosen==
- where $RSS_split$ is the sum of the RSS of the various boxes defined by each split
- this is analogous to what was done with $IG$

So in ==RT the parameters we tune are not only the best feature to split on, but also the best split value==
- while in ==DT we only tune the best feature to split on==

## Tuning the tree

### Overfitting

Since both DT and RT use a greedy alg. to create partitions, most of the times the outputted tree might be
"too bushy". This means that the decisions are taken over very few but much detailed examples, so we are
**overfitting**
- this has a ==bad impact on generalization==

![](../../../ML/overfitting1.png)

An ==hypotesis $h$ is said to overfit the training data if there exists another hypothesis $h'$ such that h has less errors than $h'$ on training data 
but a greater error on indpendent test data.==
- but how we choose sets to evaluate overfitting is a whole other topic (feature engineering)

### Pruning

Basic approaches to reduce overfitting (maybe too basic):
- ==pre pruning==: stop a tree at some point, i.e. when the support is under a certain threshold $|D_v| < k$
- ==post pruning==: grow the full tree and remove sub-trees where evidence is not enough

In both these processes, the resulting leaves are labelled with the majority class of the remaining data. Another common approach is a post-pruning + cross validation technique called ==Reduced Error Pruning==:
1. partition training data D in 2 sets: learning L and validation V
2. build a complete tree from L data
3. until accuracy on V decreases, temporarily prune non leaf nodes and replace with a a new leaf labelled with the majority class; then test again and if accuracy on V is not decreasing actually prune the tree and replace the node with a decision for the 
majority class.

![](../../../ML/learningcurve.png)

But reduced error pruning has an important side-effect: it ==wastes some of the training data, so depending on where we are on 
the learning curve we could worsen the accuracy of the model==.

Regarding RT, pruning can be done:
- stopping when the gain in RSS is under a certain threshold
- grow a very large tree an prune it
- ==complexity pruning== (not shown here)