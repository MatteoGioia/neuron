# Supervised learning

## Recall questions
      - How is the supervised ML task defined? What are the differences between classifiers and regressors?
      - What is the purpose of splitting the dataset?
      - What is the structure of a DT? How can we rewrite it? (Hint: boolean formula or set of rules)
      - What is the primary method of building a DT? 
      - How can we handle missing data when training, e.g. deciding for a feature value not in the training set?
      - Is picking a good split hard? Is the chosen method an hyperparameter?
      - What is the information gain? How is it linked to the entropy?
      - What are support and confidence?
      - What are the main issues of DT?

## Classifiers and Regressors

### Definition of the task

==Let $D$ be a dataset representing historical data about a given domain==. $D$ has a list of
==records==, called ==instances $x_i : (x_{i1}, \ldots, x_{in})$==

Each ==$x_ij$ represents a value of an attribute/feature==
- can be either continous or discrete
- one of them is the target we want to learn a prediction model for

Target is denoted with:
- ==$f(x)$ if it's a continous value==, so the model is a ==regressor==
- ==$c(x)$ if it's a discrete value==, so the model is a ==classifier==

### Training the model

Dataset is split into 2 parts:
- the ==training set==
- the ==test set==

At the end of the process, the ==created model is tested on the test set to see if the performance is accettable==.
- if not, some tuning can be done

### Geometric representation

Records can be seen as vectors in a multidimensional space

![](./static/ML/toydataset.png)

## Classification

### Decision trees

The model generated by a decision tree is a ==tree structure==. The ==root and intermediate nodes are tests on the feature values==, with as many ==branches as the possible feature values==, while each ==leaf is a decision==, so a class.

![](./static/ML/dt1.png)

A ==DT can represent any boolean function $c(x)$==. It ==can also be rewritten as a set of rules==, i.e. in Disjunctive Normal Form.

### Building a DT

The tree is built with ==greedy recursive partition== of the decision space:
- the algorithm considers each feature and builds a test node for each one
- for each test node, a new branch is created when a new feature value appears

![](./static/ML/dt2.png)

2 key aspects of building a DT (examined later on):
- ==decide for nodes for which we have 0 examples==
- learning ==best split==

The second aspect, together with learning the best ordering of the tests on the features, is actually
==learning the DT paraemeters==.

### DT pseudocode

```
   DTree(examples D, features F)
      if(all examples D are in the same category)
         return a leaf node with that category label
      else if (set of features F is empty)
         return a leaf node with the category label that is the most common in the examples subset

      Else(pick f in F)
         create node R for f
         for each(value x_i in f)
            let s_i = subset examples with value x_i for f
            add outgoing edge E from R labelled with the value of x_i
            if(s_i == empty)
               attach a leaf node to E
               label E with the category that is the most common in the examples subset
            else
               T = DTree(s_i, features F - {f})
               attach T to R
   
   Return the subtree rooted in R

```

So the first issue is solved: ==when no prior example is given, the most common category among the examples is assigned==
- in case of a tie its random

![](./static/ML/dt3.png)

We still need to see how to pick a good split.

### Information gain

Choosing ==which feature to evaluate first is key to building a minimal tree. But finding the minimal decision tree is an NP-hard problem==.
- the method used for this choice is an ==hyper-parameter==

One solution is to use ==information gain==. The idea of information gain is to ==choose first feature with high "purity"==, or for which, in simpler terms, the classification is easier.

![](./static/ML/purity.png)

==Information gain is strongly linked to entropy==:
- for example entropy is equal to 0 if all object belong to the same category
- a mixed set, instead, has entropy 1 if the samples are equally split into 2 categories

Entropy general formula: $E(D) = - \sum_{i=0}^{C} p_i log_2 (p_i)$

==Information Gain $IG$ on a feature $f$== is the ==expected reduction in entropy resulting from splitting on this feature==: $IG(D,f) =  E(D) - \sum_{v \in Values(f)} \frac{|D_v|}{D} E(D_v)$
- where $D_v$ is the subset of $D$ having value $v$ for feature $f$ (i.e. $f$=color and $v$=red)
- and the entropy of each value is weighted against its probability of appearing in the dataset

### DT Pseudocode with IG

```
   DTree(examples D, features F)
      if(all examples D are in one category)
         return a leaf node with that category label
      else if (set of features F is empty)
         return a leaf node with the category label that is the most common in the examples subset

      Else(pick the best f in F according to IG)
         create node R for f
         for each(value x_i in f)
            let s_i = subset examples with value x_i for f
            add outgoing edge E from R labelled with the value of x_i
            if(s_i == empty)
               attach a leaf node to E
               label E with the category that is the most common in the examples subset
            else
               T = DTree(s_i, features F - {f})
               attach T to R
   
   Return the subtree rooted in R

```

Notice that in the subsquent iterations the IG is calculated over the dataset where only the remaining features are taken into account

### A working example

![](./static/ML/dt5.png)

This dataset is associated with this DT. The corrisponding set of rules can also be derived.

![](./static/ML/dt6.png)

### Support and confidence

When we consider the set of rules associated with a DT, we can also look at the:
- ==support== $\frac{|D_v|}{D}$ or ==coverage represented by the set of examples that satisfy the rule==
- ==confidence==, the ==subset of $D_v$ correctly classified by the rule, which might or might not be equal to the support==

For instance, we there is no data for a set of features, if the distribution of $D_v$ is not uniform but, say, $D_{v}^{+}$ values are positive and $D_{v}^{-}$ are negative, with more positives than negatives, the support and confidence of the outputted rule will both be $\frac{|D_{v}^{+}|}{D}$

### Issues of DT

1. ==Can handle continous values but after discretization==
2. ==Only efficient== for ==large amount of training data== for data mining tasks
3. ==Noisy training data and missing value need to be handled with care==

## Regression

### Regression trees