# Transformer networks

### Recall questions

1. <details markdown=1><summary markdown="span"> What is attention and how is it used? Use the RNN translation example, leveraging context at each step. </summary>
    
    \
    

</details>


2. <details markdown=1><summary markdown="span"> What is the shape of a general attention layer? What important property does it have? Is it a good thing? </summary>
    
    \

</details>


3. <details markdown=1><summary markdown="span"> What are the key operations in the general attention layer? What is the most recent choice for al....? </summary>
    
    \
    

</details>


4. <details markdown=1><summary markdown="span"> Why is it necessary to "transform" the input features? What does it ensure? </summary>
    
    \
    We get more expressitivity

</details>


5. <details markdown=1><summary markdown="span">So what are keys, queries and values?  What is the difference between attention and self-attention?</summary>
    
    \

</details>


6. <details markdown=1><summary markdown="span"> What is positional encoding and what is its purpose? What are the desired characteristics for it? What are some possible choices?</summary>
    
    \
    "When you read positional encoding, you know exactly where you are in the sentence and how far you are from the other words"

</details>


7. <details markdown=1><summary markdown="span"> What is masked self attention and why is it necessary?</summary>
    
    \

</details>


8. <details markdown=1><summary markdown="span"> Describe the architecture of the transformer network presented by Vaswani et al. </summary>
    
    \

</details>