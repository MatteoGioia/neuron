# Exploration vs Exploitation

## Recall questions

## K-armed bandit

Problem description

### Action value function

The value of choosing an action $A$ is defined by the action value:

>$q_{*} (a) = E [R_t | A_t = a]$

### Evaluating ..

Possible ways of evaluating:
- sample mean
- incremental (stationary, non stationary)

Evaluate Q at time .. as a function of the previous actions

### Greedy action selection

An agent can choose an action:
- randomly (see image)
- using a greedy or $\varepsilon$-greedy algorithm

..

>$A_t = arg max Q_t (a)$

This method has a flaw though: there is no further exploration after the first initial choices.

$\varepsilon$-Greedy action selection

Ex 1: $0.5 + 0.5 (0.5)$

Ex 2: TODO

### Pseudocode for bandit

### Optimistic initial value

