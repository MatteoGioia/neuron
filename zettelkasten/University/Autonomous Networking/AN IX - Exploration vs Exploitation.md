# Exploration vs Exploitation

## Recall questions
	- What is the k-armed bandit problem?
	- How do we calculate the value of an action?
	- What methods can be used to evaluate the value of an action? What is the most effective?
	- How do we pick an action? What is the downside of using greedy algorithms?
	- Describe a generic k-armed bandit algorithm.
                

## K-armed bandit

The ==K-armed bandit problem will be used to formalize the scenario where an agent has to repeatedly choose among $K$ different action==. After each choice, the agent receives a reward. The objective is to maximise the expected total reward over some time. 

>K-armed bandit: at each step $t$, the agent selects an action $A$; the environment generates a reward and the goal is to maxise such reward
> - set of actions $A$
> - reward function $R$ (follows unknown prob. distr.)
> - only one state

### Action value function

The ==value of choosing an action $A$== is defined by the ==action value:==

>$q_{*} (a) = E [R_t | A_t = a]$

### Evaluating actions

Possible ways of ==evaluating:==
- ==sample average/mean estimate ==
- ==incremental (stationary, non stationary)==

In the sample average estimate, the ==value of an action $a$ is the sum of rewards observed when choosing such action divided by the total number of times it has been taken==

>$Q_t (a) = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}$   where the nominator is the sum of the rewards $R$ when $a$ is chosen before time $t$, and the denominator is the number of times $a$ is chosen before time $t$

In case the action has not been chosen before (i.e. den = 0), the quality is 0.

![](./static/AN/kband1.png)

In the case of ==incremental evaluation==, we choose the formula to use depending on if:
- the ==probability distribution of the reward is always the same: stationary==
- the ==probability distribution of therreward changes: non stationary==

>$Q_{n+1} = \frac{R_n + \ldots R_1}{n}$ estimate of the action value of an action after selecting it $n$ times

Update ==formula for stationary problem:==

>$Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]$ where $\frac{1}{n}$ is called step size

Update ==formula for non-stationary problem:== 

>$Q_{n+1} = Q_n + \alpha [R_n - Q_n]$ where $\alpha$ is constant and $\in (0,1]$ s.t. the more recent actions affect the estimate more than older rewards 

### Action selection

An ==agent can choose an action:==
- ==randomly==
- using ==a greedy or $\varepsilon$-greedy algorithm==
- using ==optimistic initial values==

==Greedy choice: pick the action with the current best estimated value==

>$A_t = arg max Q_t (a)$

The issue with ==greedy choices is that they can be suboptimal, since there is small to no further exploration of new choices. One way to fix this is to use $\varepsilon$-greedy algorithm:==

>$A_t = \cases{A_t = arg \space max \space \ Q_t (a) \space with \space probability \space 1 - \varepsilon \\ a  \sim Uniform(\{a_1 \ldots a_k\}) \space with \space probability \space \varepsilon}$

Also see: 
- Ex 1: $0.5 + 0.5 (0.5)$
- Ex 2: TODO

==Optimistic values: each action starts with the max (?) possible value and the actual valued is tuned with progressive exploration==. This method encourages exploration but also converges to the best action values.

### Pseudocode for bandit (greedy + incremental)

![](./static/AN/kbandit2.png)