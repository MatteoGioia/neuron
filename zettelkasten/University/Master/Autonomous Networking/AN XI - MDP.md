# Markov Decision Processes

## Recall questions

- What is the markov property? How can it be explained in plain terms?

![](../../../static/AN/mdp1.png)

- What is the state transition matrix?

- What is a markov process? Why is it called memoryless? What property is true for all sequence of events?

![](../../../static/AN/mdp2.png)

- What is a markov reward process?

![](../../../static/AN/mdp3.png)

- What is the return? What are the effects of changing the constant $\gamma$ and why it is used?

![](../../../static/AN/mdp4.png)

- How is the value function defined for markov reward processes?

![](../../../static/AN/mdp5.png)

- Why do we use the Bellman equation to compute the value of a state? What does it do that allows us to capture all possible decisions? 

![](../../../static/AN/mdp6.png)

How can we express the bellman equation more easily? How complex is it to solve?

![](../../../static/AN/mdp7.png)

What is a Markov Decision Process (formally)?

![](../../../static/AN/mdp8.png)

What is a policy? What is its purpose?

![](../../../static/AN/mdp9.png)

How are the new value function and action function defined? 

![](../../../static/AN/mdp10.png)

Using the last 4/5 slides as an example, compute the action values and the quality of each action.